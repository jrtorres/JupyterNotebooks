{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for testing performance of intent classification in Watson Conversation Service\n",
    "\n",
    "Modified from Conversation Performance Evaluation notebook - https://github.com/joe4k/wdcutils. Uses dataframes to capture data sets, as well as ability to get full content and build a conversation workspace.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Couple of dependencies that need to be initialized. Uses pandas for dataframes, sklearn for generating the confusion matrix and calculating performance metrics, and matplotlib to visualize the confusion matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libs/Deps\n",
    "Installs the main dependencies for the notebook. Do not run the cell if you already have the dependencies installed. You will want to restart your kernel after running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install watson-developer-cloud --upgrade\n",
    "!pip install sklearn\n",
    "\n",
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libs/Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import configparser\n",
    "import itertools \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from watson_developer_cloud import ConversationV1\n",
    "from watson_developer_cloud import WatsonException\n",
    "from watson_developer_cloud import WatsonApiException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Common Functions\n",
    "\n",
    "A set of functions used in the notebook:\n",
    "- run_test_set_df : Iterates through the dataframe content, calling Watson Conversation with utterance. Storing/returning the utterance value, the correct classification, the predicted classification and the prediction confidence.\n",
    "- plot_confusion_matrix : Uses matplotlib to visualize the confusion matrix inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_set_df(conversation, workspace_id, output_state, test_df):\n",
    "    utterance_value = []\n",
    "    actual_value = []\n",
    "    predicted_value = []\n",
    "    predicted_value_confidence=[]\n",
    "    counter=0\n",
    "    for test_data_index, test_data_row in test_df.iterrows():\n",
    "        counter += 1\n",
    "        utterance = test_data_row['utterance'] \n",
    "        try:\n",
    "            conv_response = conversation.message( workspace_id=workspace_id, input={'text':utterance}, alternate_intents=True)\n",
    "        except WatsonApiException as we:\n",
    "            print(\"Watson API Exception \", we)\n",
    "            print(test_data_index)\n",
    "            print(test_data_row)\n",
    "            raise\n",
    "        \n",
    "        if(output_state and counter % 50 == 0):\n",
    "            print(\"=======================================================================\")\n",
    "            print(\"Processed count: {0}\".format(counter))\n",
    "            print(json.dumps(conv_response, indent=4))\n",
    "            print(\"=======================================================================\")\n",
    "\n",
    "        utterance_value.append(utterance)\n",
    "        actual_value.append(test_data_row['intent'])\n",
    "        if conv_response['intents']: \n",
    "            predicted_value.append(conv_response['intents'][0]['intent'])\n",
    "            predicted_value_confidence.append(conv_response['intents'][0]['confidence'])\n",
    "        else:\n",
    "            predicted_value.append('IRRELEVANT')\n",
    "            predicted_value_confidence.append(0)\n",
    "            \n",
    "    print(\"\\nFinished processing dataframe set. {0} records\".format(counter))\n",
    "    return actual_value, predicted_value, predicted_value_confidence, utterance_value\n",
    "\n",
    "### From sklearn site - http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(conf_matrix, classes=None, normalize=False, title='', cmap=plt.cm.Blues):\n",
    "    plt.figure()\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        conf_matrix = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    thresh = conf_matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(conf_matrix.shape[0]), range(conf_matrix.shape[1])):\n",
    "        plt.text(j, i, conf_matrix[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Variables\n",
    "\n",
    "Initializes main parameters for execution. Will read from a configuration file called **notebooks_config.ini**, which should be created in the same directory as this notebook. The file should have the following structure:\n",
    "\n",
    "```\n",
    "[CONVERSATION_SERVICE]\n",
    "USERNAME = YOUR_WATSON_CONVERSATION_USERNAME\n",
    "PASSWORD = YOUR_WATSON_CONVERSATION_PASSWORD\n",
    "VERSION = 2017-05-26\n",
    "LANGUAGE = en\n",
    "WORKSPACE_ID = A WATSON CONVERSATION WORKSPACE ID\n",
    "WORKSPACE_NAME = A WATSON CONVERSATION WORKSPACE NAME\n",
    "\n",
    "[DATASET]\n",
    "ALL_DATA_CSV_FILE = A_DATA_CSV (i.e. /Users/me/alldata.csv)\n",
    "EXPERIMENT_DATA_CSV_FILE = A_SEPARATE_EXPERIMENT_DATA_CSV (i.e. /Users/me/experiment_data.csv)\n",
    "\n",
    "[OUTPUT]\n",
    "BASE_OUTPUT_DIRECTORY = A_DIRECTORY (i.e. /Users/me/tmp/)\n",
    "```\n",
    "\n",
    "At a minimum, the conversation service username/password and either a single complete data file AND/OR an experiment data file should be provided.\n",
    "\n",
    "A workspace id along with experiment data csv file is provided in cases where you already have a conversation workspace built and are just running a performance experiment.\n",
    "\n",
    "The all_data_csv_file parameter is used to generate a separate train/test set (simple withold validation) for cases where a conversation workspace is going to be created and tested.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Input Variables #################\n",
    "config_file = os.path.join(os.getcwd(), 'notebooks_config.ini')\n",
    "config = configparser.ConfigParser(allow_no_value=True)\n",
    "config.read(config_file)\n",
    "\n",
    "conversation_username = config.get('CONVERSATION_SERVICE', 'USERNAME')\n",
    "conversation_password = config.get('CONVERSATION_SERVICE', 'PASSWORD')\n",
    "conversation_version = config.get('CONVERSATION_SERVICE', 'VERSION', fallback='2017-05-26')\n",
    "conversation_language = config.get('CONVERSATION_SERVICE', 'LANGUAGE', fallback='en')\n",
    "conversation_workspace_id = config.get('CONVERSATION_SERVICE', 'WORKSPACE_ID', fallback=None)\n",
    "conversation_workspace_name = config.get('CONVERSATION_SERVICE', 'WORKSPACE_NAME', fallback='Test Workspace')\n",
    "base_output_directory = config.get('OUTPUT', 'BASE_OUTPUT_DIRECTORY', fallback = os.getcwd())\n",
    "all_data_csv_file = config.get('DATASET', 'ALL_DATA_CSV_FILE', fallback = None)\n",
    "experiment_data_csv_file = config.get('DATASET', 'EXPERIMENT_DATA_CSV_FILE', fallback = None)\n",
    "\n",
    "if all_data_csv_file is None and experiment_data_csv_file is None:\n",
    "    raise Exception('Need to specify either a full data file for training/testing or a experiment data file. Check your configuration file.')\n",
    "\n",
    "################# Generated / Internal Variables #################\n",
    "current_timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "plot_figure_size = (20,20)\n",
    "experiment_withold_amount = 0.3\n",
    "\n",
    "if all_data_csv_file is not None:\n",
    "    test_data_csv_file = base_output_directory + 'test_data_' + current_timestamp + '.csv'\n",
    "    train_data_csv_file = base_output_directory + 'train_data_' + current_timestamp + '.csv'\n",
    "    test_results_csv_file = base_output_directory + 'test_results_data_' + current_timestamp + '.csv'\n",
    "    test_confusion_matrix_csv_file = base_output_directory + 'test_confusion_matrix_' + current_timestamp + '.csv'\n",
    "    if conversation_workspace_id is not None:\n",
    "        print(\"You have supplied train/test data as well as a workspace id. A new workspace will be created with the training/test data instead of testing against the supplied workspace id.\")\n",
    "\n",
    "if experiment_data_csv_file is not None:\n",
    "    experiment_results_csv_file = base_output_directory + 'experiment_results_data_' + current_timestamp + '.csv'\n",
    "    experiment_confusion_matrix_csv_file = base_output_directory + 'experiment_confusion_matrix_' + current_timestamp + '.csv'\n",
    "\n",
    "\n",
    "conversation_wrapper = ConversationV1(\n",
    "    username=conversation_username,\n",
    "    password=conversation_password,\n",
    "    version= conversation_version\n",
    ")\n",
    "\n",
    "%whos str float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Train / Test Data\n",
    "This cell will read the all_data_csv_file into a pandas dataframe, then split the dataset into a train and test set which will be used further down to create a conversation workspace and run a performance experiment.\n",
    "\n",
    "The two generated datasets are written out as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv(all_data_csv_file)\n",
    "    print(\"Number of records: {0}\".format(len(df.index)))\n",
    "except NameError as e:\n",
    "    print(\"Error: Setup is incorrect or incomplete.\\n\")\n",
    "    raise\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size = experiment_withold_amount)\n",
    "print(\"Number of records in training Set: {0}\".format(len(train_data)))\n",
    "print(\"Number of records in withhold test Set: {0}\".format(len(test_data)))\n",
    "\n",
    "\n",
    "###### Write out test/train data #######\n",
    "train_data.to_csv(train_data_csv_file, encoding='utf-8')\n",
    "test_data.to_csv(test_data_csv_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Experiment Data\n",
    "This cell will read the experiment_data_csv_file into a pandas dataframe, so that it can be used to gather performance metrics against a conversation workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    es_df = pd.read_csv(experiment_data_csv_file)\n",
    "    print(\"Number of experiment set records: {0}\".format(len(es_df.index)))\n",
    "except NameError as e:\n",
    "    print(\"Error: Setup is incorrect or incomplete.\\n\")\n",
    "    raise\n",
    "    \n",
    "\n",
    "#Alternative, load the data frame manually.\n",
    "#df = pd.DataFrame([\n",
    "#    { \"intent\": \"intent_value_1\", \"utterance\": \"example 1\"},\n",
    "#    { \"intent\": \"intent_value_1\", \"utterance\": \"example 2\"},\n",
    "#    { \"intent\": \"intent_value_1\", \"utterance\": \"example 3\"},\n",
    "#    { intent\": \"intent_value_2\", \"utterance\": \"example 4\"},\n",
    "#    { \"intent\": \"intent_value_2\", \"utterance\": \"example 5\"}\n",
    "#], columns=['intent', 'utterance'])\n",
    "#df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Workspace Setup\n",
    "**This section is optional.**\n",
    "\n",
    "Used to create a Watson conversation workspace using the supplied data. If you already have a conversation workspace created and are just running an experiment these cells do not need to be run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Workspace\n",
    "\n",
    "Takes the training data split and builds out an intent structure that will be used to create a Watson Conversation workspace. This creates an intent model using the supplied data and also includes a single dialog node in the model that echoes the intents object identified during execution. The dialog node is not critical but incuded to remove the warning message that would appear when calling a conversation workspace with no dialog.  \n",
    "\n",
    "The created workspace id is stored for testing below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = train_data.groupby('intent')\n",
    "\n",
    "intents = []\n",
    "for name, group in grouped:\n",
    "    examples = []\n",
    "    for index, row in group.iterrows(): \n",
    "        example = { 'text': row['utterance'] }\n",
    "        examples.append(example)\n",
    "    \n",
    "    intent = { \n",
    "        'intent': name,\n",
    "        'examples': examples\n",
    "    }\n",
    "    \n",
    "    intents.append(intent)\n",
    "\n",
    "dialog_nodes = [\n",
    "    {\n",
    "     'dialog_node': 'anything_else',\n",
    "     'conditions': 'anything_else',\n",
    "     'parent': None, \n",
    "     'previous_sibling': None,\n",
    "     'output': {'text': {'values': ['<? intents ?>'], 'selection_policy': 'sequential'}}, \n",
    "     'context': None,\n",
    "     'metadata': None,\n",
    "     'go_to': None\n",
    "    }\n",
    "]\n",
    "\n",
    "response = conversation_wrapper.create_workspace(\n",
    "    dialog_nodes=dialog_nodes,\n",
    "    intents=intents,\n",
    "    language=conversation_language,\n",
    "    name=conversation_workspace_name\n",
    ")\n",
    "conversation_workspace_id = response['workspace_id']\n",
    "print(\"Workspace create response:\")\n",
    "print(json.dumps(response, indent=4))\n",
    "print(\"\\nWorkspace ID: {0}\".format(conversation_workspace_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Workspace Status\n",
    "\n",
    "If a workspace is created, need to wait for the training to complete and the model to become available. Re-run this cell until the output shows that the workspace is in an available state. The output should conclude with:\n",
    "> Status is: Available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = conversation_wrapper.get_workspace(workspace_id=conversation_workspace_id, export=False)\n",
    "print(\"Workspace details response:\")\n",
    "print(json.dumps(response, indent=4))\n",
    "print(\"\\nStatus is: {}\".format(response['status']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test / Data Experiment\n",
    "\n",
    "Following cells are used to run the actual test against the conversation service. If a train/test set was created, run the 'Execute Test Set' cell. If a Experiment set was supplied, run the 'Execute Experiment Set' cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Test Set \n",
    "\n",
    "Uses the common function to call the conversation workspace using the test set dataframe, creating a confusion matrix with the results.\n",
    "\n",
    "If output_status is set to True, the function will periodically output status of the test (how many records have been run/completed and the full watson conversation response to the current test record). If its set to False only a final output with the total number of completed test records will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_status = False\n",
    "actual_vals, predicted_vals, predicted_conf_vals, test_utterance_vals = run_test_set_df(conversation_wrapper, conversation_workspace_id, output_status, test_data)\n",
    "\n",
    "## Get Label Names\n",
    "label_names = []\n",
    "label_names = test_data['intent'].drop_duplicates().tolist()\n",
    "label_names.append('IRRELEVANT') \n",
    "\n",
    "#SKLearn Confusion Matrix\n",
    "result_confusion_matrix = confusion_matrix(actual_vals, predicted_vals, labels=label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Experiment Set\n",
    "\n",
    "Uses the common function to call the conversation workspace using the experiment dataframe, creating a confusion matrix with the results.\n",
    "\n",
    "If output_status is set to True, the function will periodically output status of the experiment (how many records have been run/completed and the full watson conversation response to the current experiment record). If its set to False only a final output with the total number of completed experiment records will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_status = False\n",
    "es_actual_vals, es_predicted_vals, es_predicted_conf_vals, es_test_utterance_vals = run_test_set_df(conversation_wrapper, conversation_workspace_id, output_status, es_df)\n",
    "## Get Label Names\n",
    "es_label_names = []\n",
    "es_label_names = es_df['intent'].drop_duplicates().tolist()\n",
    "es_label_names.append('IRRELEVANT')\n",
    "\n",
    "#SKLearn Confusion Matrix\n",
    "es_result_confusion_matrix = confusion_matrix(es_actual_vals, es_predicted_vals, labels=es_label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print / Visualize Experiment Results\n",
    "\n",
    "This section will manipulate and visualize the above test/experiment results. The first portion is against the test set results and the second section runs against the experiment set results.\n",
    "\n",
    "1. The first cell in each section creates a data frame that captures results with misses that can be written to a csv.\n",
    "2. The second cell in each section is optional and will just output the top ten records in the results dataframe. Really just a sanity check to see if labels make sense.\n",
    "3. The third cell in each section will visualize the confusion matrix in a plot.\n",
    "4. The final cell in each section will gather the performance metrics for this test (using sklearn report). * Note: You may see warning output from this cell if there is an intent in the test results that had data *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results as dataframe\n",
    "test_results = pd.DataFrame({\n",
    "     'Utterance': test_utterance_vals,\n",
    "     'Actual': actual_vals,\n",
    "     'Predicted': predicted_vals,\n",
    "     'Confidence' : predicted_conf_vals\n",
    "    }, columns=['Utterance','Actual','Predicted','Confidence', 'Missed'])\n",
    "test_results['Missed'] = test_results.apply(lambda x : 'X' if x['Actual'] != x['Predicted'] else '', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIONAL CELL - JUST A SANITY CHECK TO MAKE SURE THE RESULTS CAPTURED MAKE SENSE.\n",
    "print(\"Test Set Sample Output: \")\n",
    "test_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot/Visualize the confusion matrix\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = plot_figure_size\n",
    "plot_confusion_matrix(result_confusion_matrix, classes=label_names, title='Intent confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the performance metrics (accuracy, precision, recall, etc) of the classification test\n",
    "acc = accuracy_score(actual_vals, predicted_vals)\n",
    "print(\"Classification Overall Accuracy: {0}\\n\".format(acc))\n",
    "print(classification_report(actual_vals, predicted_vals, labels=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blind Set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results as dataframe\n",
    "es_test_results = pd.DataFrame({\n",
    "     'Utterance': es_test_utterance_vals,\n",
    "     'Actual': es_actual_vals,\n",
    "     'Predicted': es_predicted_vals,\n",
    "     'Confidence' : es_predicted_conf_vals\n",
    "    }, columns=['Utterance','Actual','Predicted','Confidence', 'Missed'])\n",
    "es_test_results['Missed'] = es_test_results.apply(lambda x : 'X' if x['Actual'] != x['Predicted'] else '', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Experiment Set Sample Output: \")\n",
    "es_test_results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot/Visualize Data\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = plot_figure_size\n",
    "plot_confusion_matrix(es_result_confusion_matrix, classes=es_label_names, title='Intent confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy of classification\n",
    "es_acc = accuracy_score(es_actual_vals, es_predicted_vals)\n",
    "print(\"Classification Overall Accuracy: {0}\\n\".format(es_acc))\n",
    "print(classification_report(es_actual_vals, es_predicted_vals, labels=es_label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Writes the result information to CSV files. One CSV file for the results containing test utterance, response and confidence. Another CSV file for the confusion matrix results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write Test Set Results to File\n",
    "test_results.to_csv(test_results_csv_file, encoding='utf-8')\n",
    "\n",
    "tmp_df_out = pd.DataFrame(data=result_confusion_matrix, index= label_names, columns=label_names)\n",
    "tmp_df_out.to_csv(test_confusion_matrix_csv_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write Blind Set Results to File\n",
    "es_test_results.to_csv(experiment_results_csv_file, encoding='utf-8')\n",
    "\n",
    "tmp_df_out = pd.DataFrame(data=es_result_confusion_matrix, index= es_label_names, columns=es_label_names)\n",
    "tmp_df_out.to_csv(experiment_confusion_matrix_csv_file, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
